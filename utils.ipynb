{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56eea7f5-285c-4ec1-a68f-626e7ee3e927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "n3pyqEKQKlkY"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sentence_transformer_model_list = ['all-mpnet-base-v2','all-distilroberta-v1','all-MiniLM-L6-v2','multi-qa-distilbert-cos-v1','paraphrase-MiniLM-L6-v2','distiluse-base-multilingual-cased-v1','distiluse-base-multilingual-cased-v2','paraphrase-multilingual-MiniLM-L12-v2','msmarco-distilbert-cos-v5','multi-qa-mpnet-base-cos-v1']\n",
    "sentence_transformer_model_list+=['thenlper/gte-large',\n",
    " 'thenlper/gte-base',\n",
    " 'intfloat/e5-base-v2',\n",
    " 'embaas/sentence-transformers-e5-large-v2',\n",
    " 'jinaai/jina-embeddings-v2-base-en']\n",
    "\n",
    "\n",
    "def embed_content(text_list, model='gemini', similarity_type='semantic_similarity', device=device):\n",
    "    print('model for embedding', model)\n",
    "    print(len(text_list))\n",
    "    if model=='gemini':\n",
    "        genai.configure(api_key=paid_key_gemini)\n",
    "        model_name=\"models/text-embedding-004\"\n",
    "        task_type = similarity_type\n",
    "        embedded_content = genai.embed_content(\n",
    "        model= model_name,\n",
    "        content=text_list,\n",
    "        task_type=task_type,\n",
    "        )['embedding']\n",
    "\n",
    "        return  embedded_content\n",
    "\n",
    "    elif model=='cohere':\n",
    "\n",
    "        embedded_content = cohere_client.embed(\n",
    "        model='embed-english-v3.0',\n",
    "        texts=text_list,\n",
    "        input_type='search_query',\n",
    "        truncate='END'\n",
    "        )\n",
    "        return embedded_content.embeddings\n",
    "\n",
    "    elif model =='text-embedding-3-small' or model =='text-embedding-3-large':\n",
    "        print('model ', model)\n",
    "        client = OpenAI(api_key=openai_api_key, organization ='org-PvvGd5GqUSJXbHLdTVVN3nzr')\n",
    "\n",
    "        batch_size = 128\n",
    "        embeddings = []\n",
    "        for i in range(0, len(text_list), batch_size):\n",
    "            print('i', i)\n",
    "            batch = text_list[i:i + batch_size]\n",
    "            response = client.embeddings.create(\n",
    "                model=model, input=batch, encoding_format=\"float\"\n",
    "            )\n",
    "            batch_embeddings = [x.embedding for x in response.data]\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "        embeddings = np.array(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "    elif model=='voyage-3-lite':\n",
    "\n",
    "        batch_size = 128\n",
    "        embeddings = []\n",
    "        for i in range(0, len(text_list), batch_size):\n",
    "            print('i', i)\n",
    "            batch = text_list[i:i + batch_size]\n",
    "            voyageai.api_key = voyage_api_key\n",
    "            vo = voyageai.Client()\n",
    "            batch_embeddings = vo.embed(batch, model=\"voyage-3-lite\", input_type=\"document\").embeddings\n",
    "            embeddings.extend(batch_embeddings)\n",
    "            # time.sleep(1)\n",
    "        return embeddings\n",
    "\n",
    "    elif model=='Mistral':\n",
    "\n",
    "        batch_size = 100\n",
    "        all_embeddings = []\n",
    "\n",
    "        print('batch_size ', batch_size)\n",
    "\n",
    "        for i in range(0, len(text_list), batch_size):\n",
    "            print('i',i)\n",
    "            print('sleep')\n",
    "            time.sleep(2)\n",
    "\n",
    "\n",
    "            batch = text_list[i:i+batch_size]\n",
    "          # In a real scenario, you would call the API here for the batch\n",
    "          # Example with a dummy return:\n",
    "          # if model == 'Mistral':\n",
    "            # Simulate a call to Mistral API\n",
    "            # import os\n",
    "            # from mistralai.client import MistralClient\n",
    "            # from mistralai.models.embeddings import EmbeddingRequest\n",
    "\n",
    "            # mistral_key = os.environ.get(\"MISTRAL_API_KEY\") # Make sure your API key is in environment variable\n",
    "            # if not mistral_key:\n",
    "                # raise ValueError(\"MISTRAL_API_KEY environment variable not set\")\n",
    "\n",
    "            client = Mistral(api_key=mistral_key)\n",
    "\n",
    "            # print(f\"Embedding batch {i//batch_size + 1}/{len(text_list)//batch_size + 1}\")\n",
    "            # print(f\"Batch size: {len(batch)}\")\n",
    "\n",
    "            try:\n",
    "                embeddings_batch_response = client.embeddings.create(\n",
    "                    model=\"mistral-embed\", # Use the correct Mistral embedding model name\n",
    "                    inputs=batch,\n",
    "                    # The API might have a different parameter name for inputs,\n",
    "                    # referring to the documentation is crucial. It seems 'inputs' was\n",
    "                    # used in the traceback, but 'input' is common in newer clients.\n",
    "                    # Let's stick to 'input' based on potential SDK updates or common practice.\n",
    "                )\n",
    "                batch_embeddings = [data.embedding for data in embeddings_batch_response.data]\n",
    "                all_embeddings.extend(batch_embeddings)\n",
    "            except Exception as e:\n",
    "                print(f\"Error embedding batch starting at index {i}: {e}\")\n",
    "                # Depending on your needs, you might want to handle errors differently,\n",
    "                # like retrying or logging the specific texts that failed.\n",
    "                # For now, we'll raise the exception to stop execution if an error occurs.\n",
    "                raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # time.sleep(10)\n",
    "\n",
    "\n",
    "        # model = \"mistral-embed\"\n",
    "        # print('text_list ', len(text_list))\n",
    "\n",
    "        # client = Mistral(api_key=mistral_key)\n",
    "\n",
    "        # embeddings_batch_response = client.embeddings.create(\n",
    "        #     model=model,\n",
    "        #     inputs=text_list,\n",
    "        # )\n",
    "\n",
    "        # print('embeddings_batch_response',embeddings_batch_response)\n",
    "\n",
    "\n",
    "        # all_embeddings_list = []\n",
    "\n",
    "        # # Iterate through the data in the response and extract each embedding vector\n",
    "        # if embeddings_batch_response.data:\n",
    "        #     for x in embeddings_batch_response.data:\n",
    "        #     # for embedding_data_object in embeddings_batch_response.data:\n",
    "        #       all_embeddings_list.append(x.embedding)\n",
    "\n",
    "\n",
    "\n",
    "        # batch_size = 128\n",
    "        # embeddings = []\n",
    "        # for i in range(0, len(text_list), batch_size):\n",
    "        #     print('i', i)\n",
    "        #     batch = text_list[i:i + batch_size]\n",
    "        #     voyageai.api_key = voyake_api_key\n",
    "        #     vo = voyageai.Client()\n",
    "        #     batch_embeddings = vo.embed(batch, model=\"voyage-3-lite\", input_type=\"document\").embeddings\n",
    "        #     embeddings.extend(batch_embeddings)\n",
    "        #     # time.sleep(1)\n",
    "        return all_embeddings# embeddings_batch_response\n",
    "\n",
    "\n",
    "\n",
    "    elif model in sentence_transformer_model_list:\n",
    "        print('sentence transformer ', model)\n",
    "        if model in sentence_transformer_models_dict:\n",
    "            model_embed = sentence_transformer_models_dict[model]\n",
    "        else:\n",
    "            print('create model object ' , model)\n",
    "            model_embed = SentenceTransformer(model, device=device)\n",
    "            sentence_transformer_models_dict[model]=model_embed\n",
    "\n",
    "        embeddings = model_embed.encode(text_list)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45a1d7bf-43ac-4635-905e-4da8594bdd78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Ww67xHYSKlkc"
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "\n",
    "\n",
    "# safety_settings_gemini=[\n",
    "# { \"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\" },\n",
    "# { \"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\" },\n",
    "# { \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\" },\n",
    "# { \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"}\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content_replacement_of_text(content, max_tokens=5000, delay=0, model='sonnet'):\n",
    "\n",
    "    print('model for deloc ', model)\n",
    "\n",
    "    if len(content)< 10:\n",
    "        print('No content ', content)\n",
    "        return \"\", \"\"\n",
    "\n",
    "\n",
    "    time.sleep(delay)\n",
    "\n",
    "\n",
    "    prompt_prefix = f'Given below text, please convert all Person names(which are Proper Nouns) to a UNIQUE ID such as CHAR_A, CHAR_B, CHAR_C etc..  Keep it unique and for each UNIQUE Person name(which is a Proper Noun) use a UNIQUE ID. DO NOT KEEP THE ORIGINAL Person Names(which are Proper Nouns) in the generated paragraph text. Next, Replace all occurences  City/Country/Village/Town/River/Continent etc. names which are PROPER NOUNS to a UNIQUE ID such as LOC_A, LOC_B, LOC_C, LOC_D etc.. Next, Replace all occurences of company/organization names which are PROPER NOUNS to a UNIQUE ID such as ORG_A, ORG_B, ORG_C, ORG_D etc..   Do not replace monument/landmark names like Eiffel tower etc. Output contains the modified text only....\\n  The text is provided below ::::\\n\\n '\n",
    "\n",
    "\n",
    "    prompt = prompt_prefix + content\n",
    "\n",
    "    prompt+='\\n\\n DO NOT GENERATE ANYTHING ELSE BEFORE OR AFTER. The output is of the format.\\ n **MODIFIED_TEXT:** '\n",
    "\n",
    "\n",
    "    # Format the request payload using the model's native structure.\n",
    "\n",
    "    if model=='sonnet':\n",
    "        native_request = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": int(max_tokens),\n",
    "            \"temperature\": 0.9,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        # Convert the native request to JSON.\n",
    "        request = json.dumps(native_request)\n",
    "\n",
    "        config = Config(read_timeout=1000)\n",
    "\n",
    "        brt = boto3.client(service_name=aws_service_name_aws,\n",
    "        region_name=aws_region_name,\n",
    "        config=config)\n",
    "\n",
    "        try:\n",
    "            # Invoke the model with the request.\n",
    "            response = brt.invoke_model(modelId=aws_model_id, body=request)\n",
    "\n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Can't invoke '{aws_model_id}'. Reason: {e}\")\n",
    "            return \"\", \"ERROR \" +str(e)\n",
    "\n",
    "        # Decode the response body.\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        response_text = model_response[\"content\"][0][\"text\"]\n",
    "        return response_text#,model_response\n",
    "\n",
    "    elif model =='gemini':\n",
    "\n",
    "        genai.configure(api_key=paid_key_gemini)\n",
    "        gemini_model = \"gemini-1.5-pro\"\n",
    "\n",
    "\n",
    "        model_object_gemini = genai.GenerativeModel(gemini_model)\n",
    "\n",
    "        response_for_prompt = model_object_gemini.generate_content(prompt.encode().decode('unicode_escape')\n",
    "\n",
    "                                                        , safety_settings = safety_settings_gemini\n",
    "\n",
    "        )\n",
    "\n",
    "        # print('response_for_prompt ', response_for_prompt)\n",
    "\n",
    "        try:\n",
    "\n",
    "            if len(response_for_prompt.candidates) >0:\n",
    "                return response_for_prompt.text\n",
    "        except:\n",
    "            print('exception ')\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content_remove_text(content, max_tokens=5000, delay=0, model='sonnet'):\n",
    "\n",
    "    print('model for deloc ', model)\n",
    "\n",
    "    if len(content)< 10:\n",
    "        print('No content ', content)\n",
    "        return \"\", \"\"\n",
    "\n",
    "    # print('content ', content)\n",
    "\n",
    "    time.sleep(delay)\n",
    "\n",
    "# person names\n",
    "# and location(cities/towns/countries etc. )\n",
    "    prompt_prefix = f'Given below text, please COMPLETELY DELETE all Person/Character names which are PROPER NOUNS and City/Country/Village/Town/River/Continent/Organization etc. names which are PROPER NOUNS etc. Wherever they occur replace with empty string. Completely remove them and not anything else. Do not delete monument/landmark names like Eiffel tower etc. Do not remove He/She/him/her etc.. Output contains the modified text only....\\n  The text is provided below ::::\\n\\n '\n",
    "\n",
    "    prompt = prompt_prefix + content\n",
    "\n",
    "    prompt+='\\n\\n DO NOT GENERATE ANYTHING ELSE BEFORE OR AFTER. The output is of the format.\\ n **MODIFIED_TEXT:** '\n",
    "\n",
    "\n",
    "\n",
    "    if model=='sonnet':\n",
    "        native_request = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": int(max_tokens),\n",
    "            \"temperature\": 0.9,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        # Convert the native request to JSON.\n",
    "        request = json.dumps(native_request)\n",
    "\n",
    "        config = Config(read_timeout=1000)\n",
    "\n",
    "        brt = boto3.client(service_name=aws_service_name_aws,\n",
    "        region_name=aws_region_name,\n",
    "        config=config)\n",
    "\n",
    "        try:\n",
    "            # Invoke the model with the request.\n",
    "            response = brt.invoke_model(modelId=aws_model_id, body=request)\n",
    "\n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Can't invoke '{aws_model_id}'. Reason: {e}\")\n",
    "            return \"\", \"ERROR \" +str(e)\n",
    "\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "        response_text = model_response[\"content\"][0][\"text\"]\n",
    "        return response_text\n",
    "\n",
    "    elif model =='gemini':\n",
    "\n",
    "        genai.configure(api_key=paid_key_gemini)\n",
    "        gemini_model = \"gemini-1.5-pro\"\n",
    "        gemini_model = 'gemini-2.0-flash'\n",
    "\n",
    "        model_object_gemini = genai.GenerativeModel(gemini_model)\n",
    "\n",
    "        print(' len text ', len(prompt))\n",
    "\n",
    "        response_for_prompt = model_object_gemini.generate_content(prompt.encode().decode('unicode_escape')\n",
    "\n",
    "                                                        , safety_settings = safety_settings_gemini\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            if len(response_for_prompt.candidates) >0:\n",
    "                return response_for_prompt.text\n",
    "        except:\n",
    "            print('exception ')\n",
    "            RaiseError('exception in Gemini Anonymization')\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "def text_anon(row, model, delay=0, REPLACEMENT_OR_REMOVAL=None):\n",
    "\n",
    "    print('REPLACEMENT_OR_REMOVAL ', REPLACEMENT_OR_REMOVAL)\n",
    "\n",
    "    if REPLACEMENT_OR_REMOVAL =='REPLACEMENT':\n",
    "        modified_text = content_replacement_of_text(content=row['text'], model=model, delay=delay)\n",
    "\n",
    "    elif REPLACEMENT_OR_REMOVAL =='REMOVAL':\n",
    "        modified_text = content_remove_text(content=row['text'], model=model, delay=delay)\n",
    "\n",
    "\n",
    "    return modified_text\n",
    "\n",
    "def get_modified_text_index(text):\n",
    "    return text.find('**MODIFIED_TEXT:**')\n",
    "\n",
    "def get_text_after_modified_text(text):\n",
    "    index = get_modified_text_index(text)\n",
    "    return text[index + len('**MODIFIED_TEXT:**'):].strip() if index != -1 else text\n",
    "\n",
    "def machine_summary_anon(index_row_tuple, model, delay=0, REPLACEMENT_OR_REMOVAL=None ):\n",
    "    print('index_row_tuple ', index_row_tuple)\n",
    "    row = index_row_tuple\n",
    "    machine_summaries = row['machine_summaries']\n",
    "    print('machine summaries ', machine_summaries  )\n",
    "\n",
    "    if REPLACEMENT_OR_REMOVAL =='REPLACEMENT':\n",
    "        replaced_summaries = [content_replacement_of_text(content=summary, model=model, delay=delay) for summary in machine_summaries]\n",
    "\n",
    "    elif REPLACEMENT_OR_REMOVAL =='REMOVAL':\n",
    "        replaced_summaries = [content_remove_text(content=summary, model=model, delay=delay) for summary in machine_summaries]\n",
    "    return replaced_summaries\n",
    "\n",
    "\n",
    "def human_summary_anon(index_row_tuple, model, delay=0, REPLACEMENT_OR_REMOVAL=None):\n",
    "    row = index_row_tuple\n",
    "    human_summaries = row['human_summaries']\n",
    "    print('human_summaries ', human_summaries)\n",
    "\n",
    "    if REPLACEMENT_OR_REMOVAL =='REPLACEMENT':\n",
    "        replaced_summaries = [content_replacement_of_text(content=summary, model=model, delay=delay) for summary in human_summaries]\n",
    "\n",
    "    elif REPLACEMENT_OR_REMOVAL =='REMOVAL':\n",
    "        replaced_summaries = [content_remove_text(content=summary, model=model, delay=delay) for summary in human_summaries]\n",
    "\n",
    "    return replaced_summaries\n",
    "\n",
    "\n",
    "\n",
    "def compute_cosine_similarities_with_text(row, model_name, deloc=False):\n",
    "    string_text_column = (f'embedding_text_{model_name}' if deloc == False\n",
    "                          else f'embedding_text_delocalized_{model_name}')\n",
    "    embedding_text = np.array(row[string_text_column])\n",
    "\n",
    "    string_machine_summary_column = (f'embedding_machine_summaries_{model_name}' if deloc == False\n",
    "                                     else f'embedding_machine_summaries_delocalized_{model_name}')\n",
    "\n",
    "    embedding_machine_summaries = [np.array(embedding) for embedding in row[string_machine_summary_column]]\n",
    "\n",
    "    similarities = {\n",
    "        'text_and_machine_summaries': [cosine_similarity([embedding_text], [embedding])[0][0] for embedding in embedding_machine_summaries],\n",
    "    }\n",
    "\n",
    "    return similarities['text_and_machine_summaries']\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "def compute_max_cosine_sim_machine_human(row, model_name, anon=False, llm_used=None):\n",
    "\n",
    "    string_machine_summary_column = (f'embedding_machine_summaries_{model_name}' if anon == False\n",
    "                                     else f'embedding_machine_summaries_anonymized_{model_name}_llm_used_{llm_used}')\n",
    "\n",
    "\n",
    "    string_human_summary_column = (f'embedding_human_summaries_{model_name}' if anon == False\n",
    "                                     else f'embedding_human_summaries_anonymized_{model_name}_llm_used_{llm_used}')\n",
    "\n",
    "    embedding_human_summaries = [np.array(embedding) for embedding in row[string_human_summary_column]]\n",
    "    embedding_machine_summaries = [np.array(embedding) for embedding in row[string_machine_summary_column]]\n",
    "\n",
    "    max_similarities = []\n",
    "    for machine_summary in embedding_machine_summaries:\n",
    "        similarities = [cosine_similarity([machine_summary], [human_summary])[0][0] for human_summary in embedding_human_summaries]\n",
    "        max_similarities.append(max(similarities))\n",
    "\n",
    "    return max_similarities\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
